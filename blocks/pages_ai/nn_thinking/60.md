# DenseNet
DenseNet：密集型网络
整体网络结构如下图
![densenet](./pages_ai/nn_thinking/res/densenet1.jpg)

## 前言
在计算机视觉领域，卷积神经网络（CNN）已经成为最主流的方法，比如最近的GoogLenet，VGG-19，Incepetion等模型。CNN史上的一个里程碑事件是ResNet模型的出现，ResNet可以训练出更深的CNN模型，从而实现更高的准确度。ResNet模型的核心是通过建立前面层与后面层之间的“短路连接”（shortcuts，skip connection），这有助于训练过程中梯度的反向传播，从而能训练出更深的CNN网络。

今天我们要介绍的是DenseNet模型，它的基本思路与ResNet一致，但是它建立的是**前面所有层**与**后面层**的**密集连接（dense connection）**，它的名称也是由此而来。**DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）**。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。

## 算法原理
![densenet](./pages_ai/nn_thinking/res/densenet2.jpg)
DenseNet的原理非常简单，就是把前阶段的特征数据进行**按通道拼接**

![densenet](./pages_ai/nn_thinking/res/densenet3.jpg)
如上图所示，在每一次的拼接之后，都进行Conv, BN, LeakyReLu，
所以就形成了一个**Dense Block**，通过这个DenseBlock可以形成n个长度的DenseNet
DenseBlock有一些特点：
1. 每一层的输出的特征图的大小相同(这样才可以在通道维度上进行拼接)
2. 每一层都可以输出k个特征图（通道数是k），但是每一次都会拼接前面的，所以通道经过几次拼接后会比较大。通常k=12，经过多次拼接，就已经很好了。但是实际上，每次，只有k个特征是独有的，其他的都是来自于之前的，所以可以实现特征重用。
![denseblock](./pages_ai/nn_thinking/res/denseblock.jpg)

到最后的输出的通道数会比较大，可以通过再加一层Conv，改变通道数，以降低特征图数量，提高计算效率。
常用的有两种减少特征数量的结构：
1. BottleNeck：BN + ReLu + 1x1Conv + BN + ReLu + 3x3Conv
2. Transition：BN + ReLu + 1x1Conv + 2x2AvgPool

## DenseNet作用
1. 由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的“deep supervision”；
2. 参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate（即k比较小），每个层所独有的特征图是比较小的；
3. 由于特征复用，最后的分类器使用了低级特征。

### 特征重用
所谓`特征重用`，网络上很多并没有讲解清楚。
因为这个涉及到Tensorflow和PyTorch的自动求导机制。
一般的框架的求导原理是，对张量进行求导。
这个张量参与了什么`算术运算`，会有一个记录，然后套用求导公式来求导。
一个张量经过N次的拼接，并没有经过`算数运算`，所以求导的结果不会发生变化，会在拼接后的新的Tensor当中保留，而不会重新计算。
所以，这就实现了特征重用，即原先的求导结果没有丢弃，依然传递在下一个张量当中。
所以，特征更多，但是计算量却不高。

## 参考文献
[DenseNet: 比ResNet更好的CNN模型](https://zhuanlan.zhihu.com/p/37189203)