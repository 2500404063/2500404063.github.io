# Dropout

## Dropout的直观表现
![dropout](./pages_ai/nn_thinking/res/dropout.png)
可见，通过Dropout使得神经网络中的一些神经元失效（即输出值为0），让网络变得稀疏。

## Dropout原理
![dropout](./pages_ai/nn_thinking/res/dropout_theory.png)
$Bernoulli$是伯努利分布，即B(p)
$B(p) \in \{0,1\}$，$p \in [0,1]$
表示有p的概率为1，1-p的概率为0
通过这个实现随机让一些神经元失效。

## 训练和预测
在训练的时候，dropout是开启的，预测时关闭。

## Dropout作用
1. 减少过拟合

## 解释
通过将神经元拆成小部分，这些小部分去学习，学的好就好，学不好还有其他部分的神经元可以接着学习。
类比一下，要组织一场爆破任务
1. 集中50人，让这50个人密切精准分工，搞一次大爆破。
2. 将50人分成10组，每组5人，分头行事，去随便什么地方搞点动作，成功一次就算。
显然第二种更好

简单地说，Dropout减弱了神经网络的联合适应性，变成部分适应，再综合，由此来增强了泛化能力。

因为联合适应性变弱，就算一部分神经元学坏了也问题不大，还有其它部分可以学习，就提高了鲁棒性（俚语来说就是耐操性）

参考文献
[1] Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.

[2] Dropout as data augmentation. http://arxiv.org/abs/1506.08700