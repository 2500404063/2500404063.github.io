# L1 L2 Regularization

对于神经网络这种黑箱，为了防止过拟合，我们引入正则化。
L1，L2正则化都是加在Loss上的惩罚项。

L1, L2指的是L1和L2范数
L1 = $||w||_1$ = $\Sigma_{i}|w_i|$
L2 = $||w||_2$ = $\Sigma_{i}{w_i^2}$

## 正则化的目的
根本目的：限制学习能力，提高模型泛化能力。
为什么降低学习能力能够提高模型的泛化能力呢？
这就要从为什么会产生过拟合(overfitting)说起。
当数据集不足，或者数据集重复度太高时最容易产生过拟合，这种情况下学习太久，loss确实在训练集上很低了，说明模型训练很好，好到与必须要与训练集一模一样才会被认为是同一类，即学习太多细节了，而不是学习到这些数据共同的特征。

解决办法：
第一种：我们可以通过**有目的的扩增数据集**，减少重复,增加多样性,注意要平衡各种类型的数据的比例，来让机器学习更加多样化的数据，从而减少过拟合。
本质是，让机器学习到一些数据的共同特征（也就是我们真正想要让机器学习的东西），而不是某一些数据独有的特征。

第二种：**限制学习**，来使得机器不要学习太细，有80%相似就可以了，不要要求和训练集的数据100%相同。

正则化就属于第二种。

正则化通过约束参数(weight)的取值范围，降低模型复杂度。

## 几何解释
![l1l2](./pages_ai/nn_thinking/res/l1l2.png)
左边的是L2正则化，右边的是L1正则化。
蓝色的同心圆表示损失函数的等高线，中间的小蓝点表示最优解。
可见L1和L2给出了一个范围，只能在这个范围内优化, 限制了学习能力，使得Loss不能降到非常非常低。

可以看到在正则化的限制之下，L2正则化给出的最优解$w∗$是使解更加靠近原点，也就是说L2正则化能降低参数范数的总和。

L1正则化给出的最优解$w∗$是使解更加靠近某些轴，而其它的轴则为0，所以L1正则化能使得到的参数稀疏化。

## L1,L2正则化的推导
ref: https://www.cnblogs.com/heguanyou/p/7582578.html
ref: https://zhuanlan.zhihu.com/p/29360425
贝叶斯算法的先验估计本质上就是一种“正则化”。
L1与拉普拉斯先验一致，L2与高斯先验一致。
通过极大似然估计我们也可以神奇地发现在最小化损失函数的时候也存在正则化。

大部分的正则化方法是在经验风险或者经验损失$L_{emp}$（emprirical loss）上加上一个结构化风险，我们的结构化风险用参数范数惩罚$Ω(θ)$用来限制模型的学习能力、通过防止过拟合来提高泛化能力。所以总的损失函数（也叫目标函数）为：
$$
J(\theta; X, y) = L_{emp}(\theta; X, y) + \alpha\Omega(\theta) \tag{1.1}
$$
其中X是输入数据，y是标签，θ是参数，$α∈[0,+∞]$是用来调整参数范数惩罚与经验损失的相对贡献的超参数，当α=0时表示没有正则化，α越大对应该的正则化惩罚就越大。对于L1正则化，我们有：

$$
Ω(θ)=∥w∥1 \tag{1.2}
$$

有没有偏置的条件下，θ就是w，结合式(1.1)与(1.2)，我们可以得到L1正则化的目标函数：
$$
J(w; X, y) = L_{emp}(w; X, y) + \alpha\|w\|_1  \tag{3.1}
$$

我们的目的是求得使目标函数取最小值的w∗，上式对w求导可得：
$$
\nabla_w J(w; X, y) = \nabla_w L_{emp}(w; X, y) + \alpha \cdot sign(w) \tag{3.2}
$$
其中若$w>0$，则$sign(w)=1$；若$w<0$，则$sign(w)=−1$；若$w=0$，则$sign(w)=0$。当$α=0$，假设我们得到最优的目标解是$w∗$，用泰勒公式在$w∗$处展开可以得到（要注意的$∇J(w∗)=0$）：
$$
J(w; X, y) = J(w^*; X, y) + \frac{1}{2}(w - w^*)H(w-w^*) \tag{3.3}
$$

其中H是关于w的Hessian矩阵，为了得到更直观的解，我们简化H，假设H这对角矩阵，则有：
$$
H = diag([H_{1,1},H_{2,2}...H_{n,n}]) \tag{3.4}
$$
将上式代入到式(3.1)中可以得到，我们简化后的目标函数可以写成这样：
$$
J(w;X,y)=J(w^*;X,y)+\sum_i\left[\frac{1}{2}H_{i,i}(w_i-w_i^*)^2 + \alpha_i|w_i| \right] \tag{3.5}
$$

从上式可以看出，w各个方向的导数是不相关的，所以可以分别独立求导并使之为0，可得：
$$
H_{i,i}(w_i-w_i^*)+\alpha \cdot sign(w_i)=0 \tag{3.6}
$$
我们先直接给出上式的解，再来看推导过程：
$$
w_i = sign(w^*) \max\left\{ |w_i^*| - \frac{\alpha}{H_{i,i}},0 \right\} \tag{3.7}
$$


## 参考文献

[1] 深入理解L1、L2正则化, MrLi, https://zhuanlan.zhihu.com/p/29360425