# CSPNet
CSPNet是在DenseNet基础上改进而来，所以读者需要先去看DenseNet。
在本文中，作者提出了跨阶段局部网络(CSPNet)，用来缓解以往工作需要从网络架构角度进行大量推理计算的问题，作者把这个问题归结为网络优化中的「**重复梯度信息**」。

CSPNet通过整合网络阶段开始和结束的特征图来尊重梯度的可变性，在我们的实验中，该方法减少了20%的计算量，在ImageNet数据集上具有相当、甚至更高的精度，并且在MS COCO目标检测数据集上的AP50方面显著优于最先进的方法。

除此之外，CSPNet易于实现，**并且足够通用**，可以处理基于「ResNet」、「ResNeXt」和「DenseNet」的体系结构。

## CSPNet思考
设计CSPNet的主要目的是使该体系结构能够实现「**更丰富的梯度组合，同时减少计算量**」。
**通过将基础层的特征图划分为两个部分，然后通过提出的跨阶段层次结构将它们合并，可以实现此目标。**

「作者的主要想法」是通过分割梯度流，**使梯度流通过不同的网络路径传播**。
通过切换串联和过渡步骤，传播的梯度信息可以具有较大的相关性差异。
此外，CSPNet可以大大减少计算量，并提高推理速度和准确性。

## CSPNet原理
CSPNet的原理还是非常简单的，但是其背后的思想不容易理解。
CSPNet的结构图如下
![CSPNet](./pages_ai/nn_thinking/res/cspnet.jpg)
从图中可见，CSPNet分为三部分：
1. 进入DenseNet
2. 直接进入尾部
3. Transition层

看起来有一种残差结构的感觉，那么这个结构到底起了什么样的作用呢？
CSPNet保留了DenseNet之前的结果，也保留了DenseNet之后的结果，
将两个结果Concat之后，梯度包含了**前后的梯度变化**
**让我们来和DenseNet进行对比**
![cspnet_1](./pages_ai/nn_thinking/res/cspnet_1.jpg)
![cspnet_2](./pages_ai/nn_thinking/res/cspnet_2.jpg)

**CSPNet的前向传播和反向传播的参数有：**
![cspnet_3](./pages_ai/nn_thinking/res/cspnet_3.jpg)
![cspnet_4](./pages_ai/nn_thinking/res/cspnet_4.jpg)

接下来，让我们来仔细思考一下CSPNet的本质原理，这里非常重要~
CSPNet数据分成两路，一路会经过一段复杂的特征提取，一路保持不变。
最后进入Transition层的是，变化前的和变化后的，
B=AX，已知A,已知B，是不是就能求出X？而**这个X其实就是第一条路当中所提取的特征**（也就是CNN的卷积核参数）
通过这种方式，Transition层就会知道第一条路到底提取了什么特征，那么再在Transition层去进行变换的时候，
就会**参考之前已经提取的特征**来变化，而不是重复学习相同的特征。
所以，CSPNet可以提高学习速率，也可以提高准确率。

## CSPNet的Transition组合探索
![transition](./pages_ai/nn_thinking/res/transition.png)
上图展示了四种Transition的处理方式。
第一种(a)：毫无意义，这就相当于是再接了一些Conv一样。
第三种(c)：直接把DenseNet的输出和变换前的特征拿来，尝试学习新的特征。
第四种(d)：只处理DenseNet的输出，把已经学习过的特征经过Transition变换，和变换前的拼接。
第二种(b)：先处理一次变换后的特征（相当于一次预处理），再和变换前的特征拼接后再处理一次，这样就实现了我们上面所说的学习新的特征，而不是旧的特征。

## CSPNet的扩展
CSPNet的适用性很广泛，本文是CSPNet和DenseNet结合的CSPDenseNet。
也可以可ResNet，CBL等结合，这种在YoLoV5当中使用很多。

## 参考
[【CSPNet】一种增强学习能力的跨阶段局部网络](https://zhuanlan.zhihu.com/p/393778545)