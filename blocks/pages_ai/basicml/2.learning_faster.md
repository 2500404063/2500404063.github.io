# Faster to learn

## Normalize:

We need to find the general best thetas. 
If $x_{11} = 50000$ and $x_{21} = 1$, the loss will decrease very very slowly.
And the learning rate needs to be small.
But If we scale the data to a similar range like [0, 1], the training will be 
fast and the learning rate can be 0.1 even bigger.

## Select a suitable learning rate:

To draw the graph.
Too small: loss decreases slowly.
Too big: loss will be increasing.
Well, I will write a function to alter the proper Learning Rate automatically.

### Make x easier to reach y.

If we use Polynomial Regression with normalization, and `y` is bigger than 1, the learning rate cannot be 0.1 or 0.01.
It is just like $k_1*0.01^3+k2*0.01^2+k3*0.01^1+k4*0.01$
But Ys are 5, 6, 10.
Like this function: $Loss(x)=(x-y)^2$
1. This means, the minimum of loss is in distance(very far away from 0(initial k))
2. This means, the initial gradient is big (>10)
3. This means, with the gradient getting smaller(even 0.001), but it still far away to get the minimum point. Training needs many times. However, learning rate cannot be too big, else see **the next topic(If learning rate is too big)**

Sometimes, we need to Normalize y. If not, I trained it a million times.

### If learning rate is too big

If learning rate is too big, it will jump over the minimum point.
Will it come back to the minimum point? yes, but merely sometimes.
Most times, it would jump back over minimum point to a higher point, 
and jump over the minimum to a higher point.
Well, you will never get the minimum point.
This means, if at the first time, it does not jump over the minimum, it is ok.
