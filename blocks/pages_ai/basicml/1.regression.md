# How do we figure?

In the consideration of big data, we plan to use one row to train.
Because that way is easy to expand to 2 rows, 3 rows...
Y_pred = np.dot(x_i, theta)
//In some classes, they use np.dot(theta. T, x_i)
//But I think mine is better.

# Least Square Method(The cost function):

Loss = $\frac{1}{2m} * \sum_i^n(y_{pred} - y_{real})^2$
//Sum: to sum up all of the records(data1, data2, data3...)
//m: the amount of data.

# gradient decline:

## Our target

We want to make loss be 0 as small as possible. But how does gradient decline work?
First, we know Least Square Method is a Quadratic Function.
Here, we assume: $loss(k, x) = (kx - y)^2$  //k is a parameter, y is the real value.
When `kx = y` , loss is 0. And at this moment, the derivative(导数) of loss(k, x) is 0.
Our target is to adjust 'k' which makes loss small.
**More specifically, we want the derivative(导数) of loss(k, x) (Gradient) to be 0.**

## Why do we differentiate(求导) loss(k, x)?

For the sake of getting the fastest way to adjust 'k', making loss small.

## The steps:

0. Originally 'k' is 0.
1. Get the current Gradient. if it's not 0(in fact, impossible to be 0), adjust k.
2. Now, if y is > 0, the Gradient should be negative. Which means the minimum point is in the front. More clearly,  `kx < y` and we need make `kx` bigger.
1. So, to make `kx` bigger, we need to make `k` bigger.
1. Set a variable 'alpha'(learning rate) to avoid `kx > y`
1. Then,  `k = k - alpha * Gradient` //This make k bigger.
For other conditions(y<0, multiple dimensions) are same.
If you cannot understand text above, just read this sentence:
The orientation of derivative points to the minimum position.