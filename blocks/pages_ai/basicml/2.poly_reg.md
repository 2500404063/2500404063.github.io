# 多项式回归和正则化

## 概念：
1. 泛化能力：能够预测的更多，范围更广。

```python

from numbers import Number
import numpy as np


def Normalize(x: np.ndarray) -> np.ndarray:
    x_t = x.T
    rows = x_t.shape[0]
    for i in range(rows):
        minimum = np.min(x_t[i])
        maximum = np.max(x_t[i])
        if minimum < maximum:
            x_t[i] = x_t[i] - minimum
            x_t[i] = x_t[i] / (maximum - minimum)
    return x_t.T


# x^n + x^n-1 + ... + x^2 + x^1 + x^0
def Polynomial_GetThetas(x: np.ndarray) -> np.ndarray:
    theta = np.zeros((x.shape[1], 1))
    return theta


def Polynomial_ProcessData(x: np.ndarray, max_index) -> np.ndarray:
    result = None
    rows = x.shape[0]
    for r in range(rows):
        sum = np.sum(x[r])
        a_row = np.array([])
        for index in range(max_index + 1):
            temp = pow(sum, max_index - index)
            a_row = np.hstack((a_row, temp))
        if result is None:
            result = a_row
        else:
            result = np.vstack((result, a_row))
    return result


# Datas: aligned by rows.
# +0.5833   -5.5833   +16.6667   -8.6667
x = np.array([[1], [4], [2], [5]])
y = np.array([[3], [6], [7], [8]])
x = Polynomial_ProcessData(x, 3)
x = Normalize(x)
y = Normalize(y)
theta = Polynomial_GetThetas(x)


def Train(x: np.ndarray, theta: np.ndarray, y: np.ndarray, rate: Number, alpha, times):
    m = x.shape[0]
    for i in range(times):
        decline = 1 / m * np.dot(x.T, np.dot(x, theta) - y)
        theta -= rate * decline
        loss = 1 / (2 * m) * np.sum(pow(np.dot(x, theta) - y, 2))
        # L1: |w|
        # L2: |w|^2
    print("Training. loss:", loss)


Train(x, theta, y, 0.1, 0.1, 5000)
print(theta)

```