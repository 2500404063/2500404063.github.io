# ANN

## The whole thoery

Every neuron is to compute `z = wx+b` and `g(z) = sigmoid(z)` (an activation funciton)

A neuron receives all inputs from the last layer.

## How to compute?

$w^1_{23}$ means layer 1, neron 2 and parameter 3.

To compute a LAYER:
$$
\left[
\begin{array}{l}
w^1_{11} & w^1_{12} & w^1_{13} \\
w^1_{21} & w^1_{22} & w^1_{23}
\end{array}
\right]
.
\left[
\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}
\right]
+
\left[
\begin{array}{l}
b_{1} \\
b_{2}
\end{array}
\right]
=
\left[
\begin{array}{l}
z_{1} \\
z_{2}
\end{array}
\right]
$$

But for multiple records?
You just need to alter it slightly.
$z_{12}$ means the second neuron's result(z) of record 1.
$$
\left[
\begin{array}{l}
w^1_{11} & w^1_{12} & w^1_{13} \\
w^1_{21} & w^1_{22} & w^1_{23}
\end{array}
\right]
.
\left[
\begin{array}{l}
x_{11} & x_{21}\\
x_{12} & x_{22}\\
x_{13} & x_{23}
\end{array}
\right]
+
\left[
\begin{array}{l}
b_{1} \\
b_{2}
\end{array}
\right]
=
\left[
\begin{array}{l}
z_{11} & z_{21} \\
z_{12} & z_{22}
\end{array}
\right]
$$

## Activation Function

1. f(z) = Sigmoid(z) ------ D=f(z)(1-f(z))
1. f(z) = tanh(z) ----- D=1 - tanh^2(z)
1. ReLu = max(0,z) ----- D=0 or 1(uh, conditions you know)
1. Leaky ReLu = max(0.01z,z)
More to see: https://zhuanlan.zhihu.com/p/122267172
(In many situations, tanh is superior to sigmoid. In fact we seldom use sigmoid unless you are doing a two classes classification.)
ReLu has the fastest speed of training, because it has not gradient->0. Mostly, Zs are bigger than 0.
In some conditions, you can use different activation function for different layers.

## BP

### The cost function

The cost function shows the whole difference to the real value.

### Update weights

We not only care about how to adjust weights, but also we care about that adjusting which weights is the most efficient way.
(Adjust the weights in neurons whose former layer's output value is higher)
This is that "Neurons that fire together wire together."
Managing this is the x. Look at the gradient decline expression, you can see g'(x) *x.
