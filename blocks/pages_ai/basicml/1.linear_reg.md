# 多元线性回归

```python

import numpy as np


def Normalize(x: np.ndarray) -> np.ndarray:
    x_t = x.T
    rows = x_t.shape[0]
    for i in range(rows):
        minimum = np.min(x_t[i])
        maximum = np.max(x_t[i])
        if minimum < maximum:
            x_t[i] = x_t[i] - minimum
            x_t[i] = x_t[i] / (maximum - minimum)
    return x_t.T


def Standardize(x: np.ndarray) -> np.ndarray:
    pass


def Linear_ProcessData(x):
    x = np.hstack((x, np.ones((x.shape[0], 1))))
    return x


def Linear_GetTheta(x):
    theta = np.zeros((x.shape[1], 1))
    return theta


def Train(x: np.ndarray, theta: np.ndarray, y: np.ndarray, rate, times):
    m = x.shape[0]  # The amount of data
    for i in range(times):
        gd = 1/m * np.dot(x.T, np.dot(x, theta) - y)
        theta -= rate * gd
    loss = 1 / (2 * m) * np.sum(pow(np.dot(x, theta) - y, 2))
    print(loss)


# y = 2x + 1
x = np.array([[1.], [2.], [3.], [4.]])
y = np.array([[3.], [5.], [7.], [9.]])
x = Linear_ProcessData(x)
x = Normalize(x)
theta = Linear_GetTheta(x)

# 求出来的结果应该是6和3，为什么不是2和1呢？因为原来的1,2,3,4经过了Normalize，变小了。
Train(x, theta, y, 0.5, 1000)
print(theta)
```