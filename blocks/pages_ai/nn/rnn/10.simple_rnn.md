# 全连接RNN网络

## 结构
循环神经网络是一种时序网络，即
I am Chinese, I speak Chinese.
将一句话分成(I, am, Chinese, I, speak, Chinese)
![rnn](./pages_ai/nn/rnn/res/rnn.jpg)
一个RNN神经元是一个全连接或者1对1连接结构，
输入：上一个神经元的全连接输出$S_{t-1}$，当前时间步长下的数据
全连接隐藏层：$S_t=Tanh(W_1*S_{t-1} + W_2*X_t)$
输出：$Softmax(S_t)$
可见，RNN是将上一个神经元的全连接的输出和当前时间的输入都当作输入。
这样一来，RNN就能够参考上文了。

**为什么要用Tanh和Softmax？**
因为Tanh∈[-1,1]，有负有正，而且数值不大，所以可以对下一层的输出产生加或者减。
Softmax是为了得到概率值，作为真正的输出，也可使用Sigmoid

## 对于RNN的数据集
输入向量：(batch_size, time_step, features)
输出向量：(batch_size, features)
输出向量默认是把最后一个RNN神经元的结果输出，所以没有time_step维度
但是可以设置把每一个RNN神经元都输出，这样就会有time_step维度
features，是one-hot格式，因为RNN的输出是概率值

## Tensorflow RNN

```python
import os
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras

# Disable GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

data = ['abc', 'dat', 'exe', 'txt', 'doc', 'out']

def data_process(data, steps):
    x = np.zeros((len(data), steps, 26))
    y = np.zeros((len(data), 26))
    time_step_count = 0
    batch_count = 0
    for txt in data:
        for j in range(steps):
            c = ord(txt[j]) - 97
            x[batch_count][time_step_count] = tf.one_hot([c], 26).numpy()
            time_step_count += 1
        c = ord(txt[steps]) - 97
        y[batch_count] = tf.one_hot([c], 26).numpy()
        batch_count += 1
        time_step_count = 0
    return x, y


def onehot2Text(x):
    arg = tf.argmax(x, axis=-1)
    output = []
    for i in arg:
        output.append(chr(97 + i.numpy()))

    return output


x, y = data_process(data, 2)
model = keras.models.Sequential([
    keras.layers.SimpleRNN(26)
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.MeanSquaredError()
)
model.fit(x, y, epochs=200)
output = model.predict(x)
output = onehot2Text(output)
print(output)
# output: ['c', 't', 'e', 't', 'c', 't']
```

## 参考文献

[1] Understanding LSTM Networks, http://colah.github.io/posts/2015-08-Understanding-LSTMs/
[2] Illustrated Guide to LSTM's and GRU's, https://www.youtube.com/watch?v=8HyCNIVRbSU