# 残差神经网络

可参考大牛的视频教程: [B站链接](https://www.bilibili.com/video/BV1bV41177ap?p=2&spm_id_from=pageDriver)

## 为何要残差

随着神经网络的层数越多，往往会出现表现变差的情况。
一个合理的解释是，
**学错了或者学坏了**
神经网络的学习方式，其实是由我们人来控制的，通过调整数据集，调整神经网络参数。
首先神经网络都是链式的，输入一定是上一级的输出，所以
1. 对于一个较为简单的神经网络，其参数较少，如果学坏了影响不大，而且容易纠正。
2. 而对于一个复杂的神经网络，其参数非常多，如果其中的某一个层没学好，或者学坏了，就会导致后面的**难以纠正**，如果前面层的设计不好，出现了梯度衰减的情况，直接就会导致后面的层**无法学习**。
那么怎么办呢？
对于深度神经网络，这些情况是不可避免的，但是能够做的就是，**学坏了，重新学**

## 残差神经网络的结构

![残差结构](https://pic1.zhimg.com/80/v2-2180e48afdee79382519d1f8a2a7dce8_720w.jpg)
ref:https://zhuanlan.zhihu.com/p/72679537

从结构上来解释残差：下一层得到的输入是前面某一层的直接结果和经过处理后的结果。
即，输入是F(x) + x，如果F(x)学坏了，那我至少还有x的结果作为保底。
有了这个结构，神经网络在学习的时候就会发生以下事情：
以F(x)和x的和作为输入，因为x的效果已经不过错了，所以F(x)进行微调
一旦F(x)微调调整不好，神经网络为了更好的效果，就会把F(x)部分调整到接近0，只保留x部分。
以此来让最终的效果好。

另一种解释是，输入是F(x)和x的和，相当于是即参考了x，又参考了F(x)，使得让原来链式的神经网络，成为一个带分支的神经网络。保留了之前未处理的函数（x），与处理后的函数（F(x)），共同决定最后的输出。

## 残差神经网络的优点
如果理解了上面的解释，那么可以知道残差神经网络的优点有以下：
1. 防止深度神经网络的梯度衰减（因为如果F(x)学坏了，可能会导致F(x)梯度=0，但是因为F(x)+x，x的梯度不为0，所以最终梯度不会为0）
2. 将链式网络变成分支网络，使得某一层的输入，是来自前面多层的输出，而不是只有上一层的输出。这个对于卷积网络非常有用，因为通过卷积操作和池化等操作相当于是对图像进行了采样，会导致分辨率降低；通过残差，将低分辨率的图像和分辨率稍微高一点的图像都作为输入，信息更多。
> 降采样的目的：降低图片分辨率，故意让图片不清晰，防止神经网络学到的信息太具体，从而产生过拟合。

## 残差神经网络的变形
残差神经网络的x不一定是将输出一模一样的给下一层，
可以将x替换成一层卷积层，但是尽量保证简洁。