# Embedding(稠密向量)

## 产生Embedding的原因
在做NLP时，每一个汉字都有一个自己的索引，范围是[0,inf]
在训练的时候，往往需要转换成one-hot形式，
如果一个汉字的索引是2000，则one-hot形式的向量会很长。
one-hot是0或者1的向量，所以中间还有很多小数没能用起来。
Embedding的目的就是为了解决上述问题。

## Embedding用法
**样例：**
文本集[[4],[32],[67]]会被映射为[[0.3,0.9,0.2],[-0.2,0.1,0,8],[0.1,0.3,0.9]]。
**Embedding定义：**
```python
keras.layers.Embedding(input_dim, 
output_dim, 
embeddings_initializer='uniform', 
embeddings_regularizer=None, 
activity_regularizer=None, 
embeddings_constraint=None, 
mask_zero=False, 
input_length=None)
```
**参数：**
- input_dim: 整数> 0。词汇表大小， i.e. maximum integer index + 1。
- output_dim: 整数>= 0. 稠密嵌入矩阵的维度，也是词语向量的维度。
- embeddings_initializer: 应用于嵌入层矩阵的初始化器。(see initializers)。
- embeddings_regularizer: 应用于嵌入层矩阵的正则化函数 (see regularizer)。
- activity_regularizer: 应用于输出层的正则化函数。 (see regularizer)。
- embeddings_constraint: 应用于嵌入层矩阵的限制函数。 matrix (see constraints).
- mask_zero: 0是否作为特定的填充值。在使用循环层 recurrent layers 时非常有用，因为循环层的输入为可变长度。如果该值为True 那么之后模型中的所有层需要支持masking否则会触发exception。 如果mask_zero 被设置为True, 那么词汇表不能使用索引值0(input_dim 的值应该等于 词汇表大小+ 1)。
- input_length: 当该值为常量时，表示一个文本词序列的长度。如果你在该层后要使用 Flatten 以及 Dense 层，该参数是必须设置的 (不设置该参数值,全连接层Dense输出形状无法计算)。**（input_length允许填写向量，且应该和input_shape的相同，但是忽略batch维度）**
**输入输出：**
输入形状：2维张量，形状为(batch_size, sequence_length)。
输出形状：3维张量，形状为(batch_size, sequence_length, output_dim)。
**例子：**
```python
>>> model = tf.keras.Sequential()
>>> model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))
>>> # The model will take as input an integer matrix of size (batch,
>>> # input_length), and the largest integer (i.e. word index) in the input
>>> # should be no larger than 999 (vocabulary size).
>>> # Now model.output_shape is (None, 10, 64), where `None` is the batch
>>> # dimension.
>>> input_array = np.random.randint(1000, size=(32, 10))
>>> model.compile('rmsprop', 'mse')
>>> output_array = model.predict(input_array)
>>> print(output_array.shape)
(32, 10, 64)
```
实际上，输入的不一定需要是一个标量，也可以是任意维度的向量。
例如输入：(2,2,2)
输出：(2,2,2,10)

## Embedding的原理
```python
def build(self, input_shape=None):
  self.embeddings = self.add_weight(
      shape=(self.input_dim, self.output_dim),
      initializer=self.embeddings_initializer,
      name='embeddings',
      regularizer=self.embeddings_regularizer,
      constraint=self.embeddings_constraint,
      experimental_autocast=False)
  self.built = True
def compute_output_shape(self, input_shape):
    if self.input_length is None:
      return input_shape + (self.output_dim,)
    else:
      # input_length can be tuple if input is 3D or higher
      if isinstance(self.input_length, (list, tuple)):
        in_lens = list(self.input_length)
      else:
        in_lens = [self.input_length]
      if len(in_lens) != len(input_shape) - 1:
        raise ValueError('"input_length" is %s, '
                         'but received input has shape %s' % (str(
                             self.input_length), str(input_shape)))
      else:
        for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):
          if s1 is not None and s2 is not None and s1 != s2:
            raise ValueError('"input_length" is %s, '
                             'but received input has shape %s' % (str(
                                 self.input_length), str(input_shape)))
          elif s1 is None:
            in_lens[i] = s2
      return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)
```
从build函数可以看出，embeddings是有一群参数的，参数的shape是(self.input_dim, self.output_dim)
并且对有initializer来初始化参数
在源代码当中，在build环节，input_shape没有被使用到，所以build的输出的output_shape是None，而output_shape的计算放在`comput_output_shape`这个函数里面。
如果input_length不填写，output_shape就是input_shape + (self.output_dim,)  **（注意，这里的 + 是list拼接）**
如果input_length填写的话，就把input_length拿来拼接
从源码上看过来，似乎不填写input_length也应该可以，只要input_shape填写了就行。
```python
  def call(self, inputs):
    dtype = backend.dtype(inputs)
    if dtype != 'int32' and dtype != 'int64':
      inputs = math_ops.cast(inputs, 'int32')
    out = embedding_ops.embedding_lookup_v2(self.embeddings, inputs)
    if self._dtype_policy.compute_dtype != self._dtype_policy.variable_dtype:
      # Instead of casting the variable as in most layers, cast the output, as
      # this is mathematically equivalent but is faster.
      out = math_ops.cast(out, self._dtype_policy.compute_dtype)
    return out
```
在call环节，具体的embedding环节被封装在embedding_lookup_v2，embedding算法也非常复杂。