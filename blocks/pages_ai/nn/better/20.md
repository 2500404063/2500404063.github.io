# Batch Normalization
批标准化也可以用来减少过拟合，L1,L2正则化需要手动调整α超参数不断优化，而BN是一个机器会自动学习的方式。

## 什么是BN
字面理解，按批进行`标准化`
我们之前在一开始的时候就对数据进行`全部的标准化`，即把所有的批次都囊括进来计算。
常见的标准化方式：
1. 线性：把RGB图像整体/255，得到0-1的结果
2. min max方法：(a - min) / (max - min)
3. 均值与方差方法：把所有数据归到均值为0，方差为1的分布中，即确保最终得到的数据均值为0，方差为1。

而BN就是不对所有的数据进行标准化，而是选择1000条数据当中的几十条进行标准化，但是这样有什么用呢？

## BN的原理
BN是一个网络层，内置参数，机器会学习该参数，怎么实现的呢？
BN的输入是上一级网络的输出，
BN的输出：
**对于训练时：**
$$
\frac{\gamma * (batch - average(batch))}{\sqrt{(batch + \epsilon)}} + \beta
$$
其中均值、方差分别是**该批次内**数据相应维度的均值与方差，$\gamma$和$\beta$是机器会自动学习的参数，$\epsilon$是调整参数
根据均值方差方法，BN层的作用就是通过学习参数，使得把输入的数据标准化成均值为0，方差为1的输出，即输出值范围[-1,1]
对于训练的时候，这两个参数会进行改变，
**对于预测时：**
当预测的时候，预测值会直接拿训练好的$\gamma$和$\beta$参数；而均值、方差是基于**所有批次的期望**计算所得。但是这并不需要把所有的数据加载到内存然后计算，因为在训练的过程中就会通过`滑动平均(moving_mean、moving_var)`的方式，来计算出预测时所需要的均值和方差。

## BN有什么用？
1）防止梯度弥散：加快训练速度，这样我们就可以使用较大的学习率来训练网络。
为什么可以防止梯度弥散呢？
举例Sigmoid函数，当x太大的时候，函数值已经是1了，再大下去也没有意义，而且太大的话梯度就趋向0了，但是通过BN，将数值限制在[-1,1]，这样即可实现防止梯度消失。

2）提高网络的泛化能力
BN有效是因为用上BN层之后可以使用更大的学习率，从而跳出不好的局部极值，增强泛化能力，在它们的研究中做了大量的实验来验证。

还有一种说法是，BN在进行批量标准化的时候，平均值和方差是来自于一个mini batch的，但是训练的时候去一个batch中的一条进行训练，其他的同一个batch的不同数据所产生的值就成了惩罚项（正则项），即训练一条数据的时候可以看到其他的数据的平均值和方差，所以就更加具有全局性，整体性。

## 使用BN要注意

1. batch_size不能太小(>=32)，太小的话，和全局的均值和方差可能太大
2. 对于RNN等时序网络，因为输入的长度不同（句子长度不同），不能使用BN，可以使用`LayerNormalization`

## 参考文献
[1] Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift